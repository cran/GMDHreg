<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Manuel Villacorta Tilve" />

<meta name="date" content="2024-01-23" />

<title>GMDHreg: an R Package for GMDH Regression</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>







<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">GMDHreg: an R Package for GMDH
Regression</h1>
<h4 class="author">Manuel Villacorta Tilve</h4>
<h4 class="date">2024-01-23</h4>



<div id="introduction." class="section level1">
<h1>Introduction.</h1>
<p>GMDH (Group Method of Data Handling) was originated in 1968 by
Prof. Alexey G. Ivakhnenko in the Institute of Cybernetics in Kiev.</p>
<p>GMDHreg has been designed for regression analysis. For this porpouse,
it includes the next algorithms: Combinatorial, MIA (Multilayered
Iterative Algorithm), Combinatorial with Active Neurons, and GIA
(Generalized Iterative Algorithm).</p>
<p>Following a heuristic and Perceptron type approach, Ivakhnenko
attempted to resemble the Kolmogorov-Gabor polynomial by using low order
polynomials. GMDH performs sorting-out of gradually complicated
polynomial models and selecting the best solution using so called
<em>external criteria</em>. So that, GMDH algorithms give us the
possibility to find automatically interrelations in data, to select an
optimal structure of model or network, by means of this external
criteria.</p>
<p>To estimate the polynomials coefficients, it is used the least
squares method. In this aspect, GMDHreg employs Singular Value
Decomposition (SVD) to avoid singularities.</p>
<div id="external-criteria." class="section level2">
<h2>External Criteria.</h2>
<p>It is one of the key features of GMDH, describing some requirements
to the model, like the optimal structure. It is always calculated with a
separate part of data that have not been used for estimation of
polynomial coefficients. GMDHreg give us three possibilities:</p>
<ul>
<li>PRESS: <em>Predicted Residual Error Sum of Squares.</em> It take
into account all information in data sample and it is computed without
recalculating of system for each test point.</li>
<li>test: It is the classical aproach, dividing initial data in training
and testing subsamples. The training subsample is used to derive
estimates for the coefficients of the polynomials, and the test
subsample is used to choose the structure of the optimal model.</li>
<li>ICOMP: <em>Index of Informational Complexity.</em> Like PRESS, it is
computed without recalculating of system.</li>
</ul>
</div>
</div>
<div id="gmdh-combinatorial-gmdh-combi." class="section level1">
<h1>GMDH Combinatorial (GMDH-COMBI).</h1>
<p>This is the basic GMDH algorithm. It uses an input data sample
containing <span class="math inline">\(N\)</span> rows of observations
and <span class="math inline">\(M\)</span> predictor variables.</p>
<p>This algorithm generates models of all possible input variable
combinations and selects a final best model from the generated set of
models according to a chosen selection criterion. The coefficients <span class="math inline">\(\beta_i\)</span> of these models are determined by
linear regression and are unique for each neuron.</p>
<p>Be careful with this. At GMDHreg, if you select original Ivakhnenko
quadratic polynomial (G = 2) and <span class="math inline">\(M &gt;
4\)</span>, the computational time could be very expensive, even the
algorithm could never be calculated. For <span class="math inline">\(M&#39;\)</span> input variables, GMDH
Combinatorial generate <span class="math inline">\(2^{(M&#39;-1)}\)</span> models (e.g. <span class="math inline">\(M = 5 =&gt; M&#39; = 20\)</span> then you have
<span class="math inline">\(2^{19}\)</span> models to estimate).</p>
<p>Suposse you have two variables <span class="math inline">\(x_1\)</span> <span class="math inline">\(x_2\)</span> and a response <span class="math inline">\(y\)</span>. The models to estimate are <span class="math inline">\(2^5\)</span>:</p>
<p><span class="math inline">\(y = \beta_{0} +
\beta_{1}x_{1}\)</span></p>
<p><span class="math inline">\(y = \beta_{0} +
\beta_{1}x_{2}\)</span></p>
<p><span class="math inline">\(...\)</span></p>
<p><span class="math inline">\(y = \beta_{0} + \beta_{1}x_{1} +
\beta_{2}x_{2} + \beta_{3}x_{1}^2 + \beta_{4}x_{2}^2 +
\beta_{5}x_{1}x_{2}\)</span></p>
<p>Once all these models are estimated, the one with the best external
criteria is selected.</p>
<pre><code>data(airquality)

# Remove NAs
BD &lt;- airquality[complete.cases(airquality), ]

y &lt;- matrix(data = BD[, &quot;Ozone&quot;], ncol = 1)
x &lt;- as.matrix(BD[, c(&quot;Solar.R&quot;, &quot;Wind&quot;, &quot;Temp&quot;)])

# Select the training and testing set
set.seed(123)
sel &lt;- sample(1:nrow(x), size = 100)

x.train &lt;- x[sel, ]
y.train &lt;- y[sel, ]
x.val &lt;- x[-sel, ]
y.val &lt;- y[-sel, ]

# Fitting a simple linear regression for benchmark proposes
mod.lm &lt;- lm(y.train ~., data = as.data.frame(x.train))
fit.lm &lt;- predict(mod.lm, as.data.frame(x.val))
error.lm &lt;- summary(abs(fit.lm - y.val) / y.val)

mod.combi &lt;- gmdh.combi(X = x.train, y = y.train, criteria = &quot;PRESS&quot;, G = 2)
fit.combi &lt;- predict(mod.combi, x.val)
error.combi &lt;- summary(abs(fit.combi - y.val) / y.val)
</code></pre>
</div>
<div id="gmdh-combinatorial-with-active-neurons.-gmdh-twice-multilayered-combinatorial-gmdh-tmc." class="section level1">
<h1>GMDH Combinatorial with Active Neurons. GMDH Twice-Multilayered
Combinatorial (GMDH-TMC).</h1>
<p>It is an extension of Combinatorial algorithm. It constructs the
first layer of neurons in the network like GMDH-COMBI. Then, the
algorithm determines how accurate the predictions will be for all
neurons.</p>
<p>At a second stage, with <span class="math inline">\(M_2\)</span> best
neurons as regressors, GMDH-TMC repeats the proces for a second, a
third,… {i} layer, as long as this decreases external criteria
value.</p>
<p>At GMDHreg, <span class="math inline">\(M_i = M\)</span>, and it does
not implement the orginal idea of Ivakhnenko who introduces a
feed-forward system. Note that this is computationally very expensive,
especially if G = 2.</p>
<pre><code>mod.combi.twice &lt;- gmdh.combi.twice(X = x.train, y = y.train, criteria = &quot;PRESS&quot;, G = 2)
fit.combi.twice &lt;- predict(mod.combi.twice, x.val)
error.combi.twice &lt;- summary(abs(fit.combi.twice - y.val) / y.val)
</code></pre>
</div>
<div id="gmdh-multilayered-iterative-algorithm-gmdh-mia." class="section level1">
<h1>GMDH Multilayered Iterative Algorithm (GMDH-MIA).</h1>
<p>The basis of GMDH-MIA is that each neuron in the network receives
input from exactly two other neurons, except neurons at input layer. The
two inputs are then combined to produce a partial descriptor based on
the simple quadratic transfer function (a quadratic polynomial):</p>
<p><span class="math inline">\(y_{i} = \beta_{0} + \beta_{1}x_{1,i} +
\beta_{2}x_{2,i} + \beta_{3}x_{1,i}^2 + \beta_{4}x_{2,i}^2 +
\beta_{5}x_{1,i}x_{2,i}\)</span></p>
<p>where coefficients <span class="math inline">\(\beta_i\)</span> are
determined by linear regression and are unique for each neuron.</p>
<p>The network of polynomials is constructed one layer at a time. The
first network layer consists of the functions of each possible pair of
<span class="math inline">\(n\)</span> input variables resulting in
<span class="math inline">\(n(n−1)/2\)</span> neurons. The second layer
is created using inputs from the first layer and so on.</p>
<p>Due to the exponential neurons growth, after finishing the layer, a
limited number of best neurons is selected and the other neurons are
removed from the network. GMDH-MIA repeat the proces for third, fourth,…
layer, as long as decreases external criterion value.</p>
<pre><code>mod.mia &lt;- gmdh.mia(X = x.train, y = y.train, prune = 150, criteria = &quot;PRESS&quot;)
fit.mia &lt;- predict(mod.mia, x.val)
error.mia &lt;- summary(abs(fit.mia - y.val) / y.val)
</code></pre>
</div>
<div id="gmdh-generalized-iterative-algorithm-gmdh-gia." class="section level1">
<h1>GMDH Generalized Iterative Algorithm (GMDH-GIA).</h1>
<p>This algorithm is similar to GMDH-MIA, but (1) each neuron is an
active unit, its output is automatically selected thanks to GMDH-COMBI
and, (2) GMDH-GIA works with a feed-forward system, where initial
regressors are used at all layers to avoid information lose.</p>
<pre><code>mod.gia &lt;- gmdh.gia(X = x.train, y = y.train, prune = 10, criteria = &quot;PRESS&quot;)
fit.gia &lt;- predict(mod.gia, x.val)
error.gia &lt;- summary(abs(fit.gia - y.val) / y.val)
</code></pre>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
